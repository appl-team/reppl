"""NOTE: Initial version generated by cursor. May not runnable."""

import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from appl import AIRole, SystemMessage, gen, grow, ppl
from appl.compositor import Tagged
from pydantic import BaseModel


@dataclass
class Feedback:
    content: str
    score: float = 0.8  # Initial score


@dataclass
class Exemplar:
    text: str
    label: str
    solution: str
    score: float = 0.8  # Initial score


@dataclass
class FeedbackMemory:
    feedbacks: List[Feedback] = field(default_factory=list)
    temperature: float = 0.5
    threshold: float = 0.3
    beta: float = 0.2

    def add_feedback(self, feedback: str):
        """Add new feedback to memory"""
        self.feedbacks.append(Feedback(content=feedback))

    def retrieve_feedbacks(self, n: int = 3) -> List[str]:
        """Retrieve top n feedbacks based on scores"""
        if not self.feedbacks:
            return []

        scores = np.array([f.score for f in self.feedbacks])
        probs = np.exp(scores / self.temperature)
        probs = probs / np.sum(probs)

        indices = np.random.choice(
            len(self.feedbacks),
            size=min(n, len(self.feedbacks)),
            p=probs,
            replace=False,
        )
        return [self.feedbacks[i].content for i in indices]

    def update_scores(self, feedbacks: List[str], improved: bool):
        """Update feedback scores based on performance"""
        for fb in self.feedbacks:
            if fb.content in feedbacks:
                if improved:
                    fb.score = (1 - self.beta) * fb.score + self.beta
                else:
                    fb.score = (1 - self.beta) * fb.score

        # Remove low scoring feedbacks
        self.feedbacks = [f for f in self.feedbacks if f.score >= self.threshold]


@dataclass
class ExemplarFactory:
    exemplars: List[Exemplar] = field(default_factory=list)
    temperature: float = 0.5
    threshold: float = 0.3
    beta: float = 0.2

    def add_exemplar(self, text: str, label: str, solution: str):
        """Add new exemplar to factory"""
        self.exemplars.append(Exemplar(text=text, label=label, solution=solution))

    def retrieve_exemplars(self, n: int = 5) -> List[Tuple[str, str, str]]:
        """Retrieve top n exemplars based on scores"""
        if not self.exemplars:
            return []

        scores = np.array([e.score for e in self.exemplars])
        probs = np.exp(scores / self.temperature)
        probs = probs / np.sum(probs)

        indices = np.random.choice(
            len(self.exemplars),
            size=min(n, len(self.exemplars)),
            p=probs,
            replace=False,
        )
        return [
            (
                self.exemplars[i].text,
                self.exemplars[i].label,
                self.exemplars[i].solution,
            )
            for i in indices
        ]

    def update_scores(self, exemplars: List[Tuple[str, str, str]], improved: bool):
        """Update exemplar scores based on performance"""
        for e in self.exemplars:
            if (e.text, e.label, e.solution) in exemplars:
                if improved:
                    e.score = (1 - self.beta) * e.score + self.beta
                else:
                    e.score = (1 - self.beta) * e.score

        # Remove low scoring exemplars
        self.exemplars = [e for e in self.exemplars if e.score >= self.threshold]


class ExemplarResponse(BaseModel):
    text: str
    label: str
    solution: str


class FeedbackResponse(BaseModel):
    feedback: str


@ppl
def get_exemplars_and_feedback(
    prompt: str,
    error_samples: List[Dict],
    num_exemplars: int = 4,
    num_feedbacks: int = 3,
):
    """Get exemplars and feedback using the instructive meta-prompt"""

    with Tagged("meta-prompt"):
        grow(
            "I'm trying to write and complete a zero-shot classifier prompt from difficult or erroneous examples, "
            "'text' field means model input, 'label' field means true label."
        )
        grow(f"The current prompt is: {prompt}")
        grow("But this prompt gets the following examples wrong:")
        for sample in error_samples:
            grow(f"Text: {sample['text']}")
            grow(f"Label: {sample['label']}")

        grow(
            f"To improve my understanding and performance, I would like to identify {num_exemplars} "
            "typical examples from the above cases where the current prompt fails."
        )
        grow("These examples should be diverse to cover a range of different issues.")
        grow(
            "For each example, provide the following format in JSON and wrap each example with "
            "<key_example> and </key_example>:"
        )
        grow("<key_example>")
        grow("{")
        grow('  "text": "{{text}}",')
        grow('  "label": "{{label}}",')
        grow(
            '  "solution": "How to solve this problem step-by-step to get a more accurate answer."'
        )
        grow("}")
        grow("</key_example>")

        grow(
            f"After identifying these {num_exemplars} typical examples, please provide {num_feedbacks} "
            "reasons why the prompt could have gotten these examples wrong. Wrap each reason with "
            "<feedback> and </feedback>."
        )

    response = gen()

    # Parse exemplars and feedback from response
    exemplars = []
    feedbacks = []

    # Extract exemplars between <key_example> tags
    import re

    exemplar_matches = re.finditer(
        r"<key_example>(.*?)</key_example>", str(response), re.DOTALL
    )
    for match in exemplar_matches:
        try:
            exemplar_dict = json.loads(match.group(1))
            exemplars.append(ExemplarResponse(**exemplar_dict))
        except:
            continue

    # Extract feedback between <feedback> tags
    feedback_matches = re.finditer(
        r"<feedback>(.*?)</feedback>", str(response), re.DOTALL
    )
    for match in feedback_matches:
        feedbacks.append(FeedbackResponse(feedback=match.group(1).strip()))

    return exemplars, feedbacks


@ppl
def optimize_prompt(prompt: str, error_samples: List[Dict], feedbacks: List[str]):
    """Generate optimized prompt using feedback"""

    with Tagged("meta-prompt"):
        grow(
            "I'm trying to write and complete a zero-shot classifier prompt from difficult or erroneous examples, "
            "'text' field means model input, 'label' field means true label."
        )
        grow(f"The current prompt is: {prompt}")
        grow("But this prompt gets the following examples wrong:")
        for sample in error_samples:
            grow(f"Text: {sample['text']}")
            grow(f"Label: {sample['label']}")

        grow("Based on these examples the problem with this prompt is that:")
        for fb in feedbacks:
            grow(fb)

        grow(
            "Based on the above information, I refine the prompt to make the model predict correctly."
        )
        grow(
            "The refined prompt is wrapped with <prompt> and </prompt>, less that 512 words:"
        )

    response = str(gen())

    # Extract prompt between <prompt> tags
    import re

    match = re.search(r"<prompt>(.*?)</prompt>", response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return prompt


def load_data(data_path: str) -> List[Dict]:
    """Load data from jsonl file"""
    data = []
    with open(data_path) as f:
        for line in f:
            data.append(json.loads(line))
    return data


class ERM:
    def __init__(
        self,
        initial_prompt: str,
        feedback_memory: Optional[FeedbackMemory] = None,
        exemplar_factory: Optional[ExemplarFactory] = None,
    ):
        self.prompt = initial_prompt
        self.feedback_memory = feedback_memory or FeedbackMemory()
        self.exemplar_factory = exemplar_factory or ExemplarFactory()

    def optimize(
        self,
        train_data: List[Dict],
        eval_data: List[Dict],
        num_steps: int = 10,
        eval_freq: int = 2,
    ):
        """Main optimization loop"""

        best_prompt = self.prompt
        best_score = self.evaluate(eval_data)
        print(f"Initial score: {best_score}")

        for step in range(num_steps):
            # Get error samples
            error_samples = self.get_error_samples(train_data)
            if not error_samples:
                print("No errors found, optimization complete")
                break

            # Get exemplars and feedback
            exemplars, feedbacks = get_exemplars_and_feedback(
                self.prompt, error_samples
            )

            # Store exemplars and feedback
            for ex in exemplars:
                self.exemplar_factory.add_exemplar(ex.text, ex.label, ex.solution)
            for fb in feedbacks:
                self.feedback_memory.add_feedback(fb.feedback)

            # Retrieve stored feedback
            stored_feedbacks = self.feedback_memory.retrieve_feedbacks()

            # Generate new prompt
            new_prompt = optimize_prompt(self.prompt, error_samples, stored_feedbacks)
            self.prompt = new_prompt

            # Evaluate periodically
            if (step + 1) % eval_freq == 0:
                score = self.evaluate(eval_data)
                print(f"Step {step + 1}, Score: {score}")

                # Update memory scores
                improved = score > best_score
                self.feedback_memory.update_scores(stored_feedbacks, improved)

                if improved:
                    best_score = score
                    best_prompt = new_prompt

        self.prompt = best_prompt
        return best_score

    def get_error_samples(self, data: List[Dict], max_samples: int = 5) -> List[Dict]:
        """Get samples where current prompt produces incorrect answers"""
        error_samples = []
        for sample in data:
            # TODO: Implement actual prediction using task model
            pred = "dummy_prediction"
            if pred != sample["label"]:
                error_samples.append(sample)
                if len(error_samples) >= max_samples:
                    break
        return error_samples

    def evaluate(self, data: List[Dict]) -> float:
        """Evaluate current prompt on data"""
        # TODO: Implement actual evaluation using task model
        return 0.8  # Dummy score for now


def main():
    # Load data
    train_data = load_data("../../data/bbh/navigate/train.jsonl")
    test_data = load_data("../../data/bbh/navigate/test.jsonl")

    # Initial prompt from paper
    initial_prompt = """
    ## Task
    Solve the math problem.
    ## Prediction
    Text: {input}
    Label:
    """

    # Initialize ERM
    erm = ERM(initial_prompt=initial_prompt)

    # Run optimization
    final_score = erm.optimize(train_data, test_data)
    print(f"Final score: {final_score}")
    print(f"Final prompt:\n{erm.prompt}")


if __name__ == "__main__":
    main()
